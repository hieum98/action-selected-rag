import re
from typing import List, Union
import time
from agents.llm_agents import LLMAgent
from prompts import (
    evaluate,
    generate_direct_answer,
    generate_one_next_step,
    generate_subquestion_and_answer,
    reanswer_subquestion,
    rephase_question,
    generate_query,
)


class Generator:
    def __init__(
            self, 
            online_model_kwargs=None, 
            offline_model_kwargs=None,
            generate_kwargs={},
            use_cache: bool = True,
            cache_dir: str = 'cache/llm_agents',
            verbose: bool = False
            ):
        assert online_model_kwargs is not None or offline_model_kwargs is not None, "At least one model configuration must be provided."
        if online_model_kwargs is not None:
            assert all(key in online_model_kwargs for key in ['model_name', 'url']), "Online model configuration must include 'model_name' and 'url'."
            online_model_kwargs.update(generate_kwargs)
        if offline_model_kwargs is not None:
            assert all(key in offline_model_kwargs for key in ['model_name', 'agent_type']), "Offline model configuration must include 'model_name' and 'agent_type'."
            offline_model_kwargs.update(generate_kwargs)
                
        self.llm_agent = LLMAgent(
            online_model_kwargs=online_model_kwargs,
            offline_model_kwargs=offline_model_kwargs,
            use_cache=use_cache,
            cache_dir=cache_dir,
        )
        self.verbose = verbose
        if self.verbose:
            print(f"Generator initialized with online model: {online_model_kwargs} and offline model: {offline_model_kwargs}")

        # Initialize prompts
        self.evaluate_answer_prompt = evaluate.EVALUATE_ANSWER_PROMPT
        self.eval_examples = None  # Placeholder for direct answer examples
        
        self.generate_direct_answer_prompt = generate_direct_answer.DIRECT_ANSWER_PROMPT
        self.direct_answer_examples = None

        self.generate_one_next_step_prompt = generate_one_next_step.ONE_NEXT_STEP_PROMPT
        self.one_next_step_examples = None

        self.generate_subquestion_prompt = generate_subquestion_and_answer.GENERATE_SUBQUESTION_PROMPT
        self.generate_subquestion_examples = None

        self.generate_subquestion_and_answer_prompt = generate_subquestion_and_answer.SUBQUESTION_AND_ANSWER_PROMPT
        self.subquestion_and_answer_examples = None
        
        self.reanswer_subquestion_prompt = reanswer_subquestion.REANSWER_PROMPT
        self.reanswer_subquestion_examples = None

        self.rephase_question_prompt = rephase_question.REPHRASE_QUESTION_PROMPT
        self.rephase_question_examples = None

        self.evaluated_retrieved_document_prompt = evaluate.EVALUATE_RETRIEVED_DOCUMENT_PROMPT
        self.evaluated_retrieved_document_examples = None

        self.evaluate_same_question_prompt = evaluate.EVALUATE_SAME_QUESTION_PROMPT
        self.evaluate_same_question_examples = None

        self.evaluate_answer_given_context_prompt = evaluate.EVALUATE_ANSWER_GIVEN_CONTEXT_PROMPT
        self.evaluate_answer_given_context_examples = None

        self.score_reasoning_prompt = evaluate.EVALUATE_REASONING_PROMPT
        self.score_reasoning_examples = None

        self.generate_queries_prompt = generate_query.GENERATE_QUERIES_PROMPT
        self.generate_queries_examples = None

    def evaluate_answer(self, question: str, correct_answer: Union[str, List[str]], predicted_answer: str):
        """
        Evaluates the quality of a predicted answer against a correct answer.
        
        Args:
            question (str): The question being answered.
            correct_answer (str): The correct answer to the question.
            predicted_answer (str): The answer generated by the model to be evaluated.
        
        Returns:
            dict: A dictionary containing the evaluation result, the predicted answer, and the correct answer.
        """
        user_message = self.evaluate_answer_prompt.format(
            examples=self.eval_examples if self.eval_examples else "",
            question=question, 
            correct_answer=correct_answer, 
            predicted_answer=predicted_answer
        )
        messages = [{'role': 'user', 'content': user_message}]
        if self.verbose:
            print("*"* 10)
            print(f"Evaluating answer with user message: {user_message}")
        agent_input = {
            'messages': messages,
            'json_schema': evaluate.EvaluateAnswerOutput,
            'index': 0
        }
        start_time = time.time()
        responses = self.llm_agent.generate(agent_input)['output']
        if self.verbose:
            print(f"Time taken to evaluate answer: {time.time() - start_time:.2f} seconds")
        results = []
        confidence = []
        reasoning = []
        for response in responses:
            response_object = response.get('output', None)
            if response_object is not None:
                if isinstance(response_object, evaluate.EvaluateAnswerOutput):
                    answer_status = response_object.decision
                    answer_confidence = response_object.confidence
                    answer_confidence = answer_confidence.lower() if confidence else None
                    if answer_confidence == 'high':
                        confidence.append(1)
                    elif answer_confidence == 'medium':
                        confidence.append(0.5)
                    else:
                        confidence.append(0)
                    results.append(answer_status)
                    reasoning.append(response_object.reasoning)
                else:
                    print(f"Warning: Response object is not of type EvaluateAnswerOutput: {response_object}")
        # Majority voting for the results
        if len(results) > 0:
            final_result = sum(results) / len(results)  # Calculate the confidence level
            assert 0 <= final_result <= 1, "final_result should be between 0 and 1"
            final_result = final_result >= 0.5 # True if more than half of the responses are 'matched'
            confidence = sum(confidence) / len(confidence) if confidence else 0.0  # Average confidence
        else:
            # Print out to help debug the issue
            print(f"Warning: No valid responses received for evaluation")
            print(f"user_message: {user_message}")
            print(f"Responses: {responses}")
            final_result = False
            confidence = 0.0

        if self.verbose:
            print(f"Final evaluation result: {final_result}")
            print(f"Predicted answer: {predicted_answer}")
            print(f"Correct answer: {correct_answer}")
            print(f"Confidence: {confidence:.2f}")
            print(f"Reasoning: {reasoning}")
        return {
            'result': final_result,  # bool: True if the predicted answer is matched with the correct answer, False otherwise
            'predicted_answer': predicted_answer,  # str: The predicted answer to be evaluated
            'correct_answer': correct_answer,  # str or List[str]: The correct answer to the question
            'confidence': confidence,  # float: Confidence level of the evaluation result
        }
    
    def batch_evaluate_answer(self, questions: List[str], correct_answers: List[Union[str, List[str]]], predicted_answers: List[str]):
        assert len(questions) == len(correct_answers) == len(predicted_answers), "Questions, correct answers, and predicted answers must have the same length."
        messages = []
        for q, ca, pa in zip(questions, correct_answers, predicted_answers):
            user_message = self.evaluate_answer_prompt.format(
                examples=self.eval_examples if self.eval_examples else "",
                question=q, 
                correct_answer=ca, 
                predicted_answer=pa
            )
            messages.append([{'role': 'user', 'content': user_message}])
        start_time = time.time()
        responses = self.llm_agent.batch_generate(batch=messages, response_object=evaluate.EvaluateAnswerOutput, n=1) # For batch processing, do not use multiple sampling
        if self.verbose:
            print(f"Time taken to batch evaluate answers: {time.time() - start_time:.2f} seconds")
        results = []
        confidence = []
        reasoning = []
        for response in responses:
            response_object = response['output'][0].get('output', None)
            assert response_object is not None, "Response object should not be None"
            if isinstance(response_object, evaluate.EvaluateAnswerOutput):
                if self.verbose:
                    print(f"Response object: {response_object}")
                answer_status = response_object.decision
                answer_confidence = response_object.confidence
                answer_confidence = answer_confidence.lower() if answer_confidence else None
                if answer_confidence == 'high':
                    confidence.append(1)
                elif answer_confidence == 'medium':
                    confidence.append(0.5)
                else:
                    confidence.append(0)
                results.append(answer_status)
                reasoning.append(response_object.reasoning)
            else:
                print(f"Warning: Response object is not of type EvaluateAnswerOutput: {response_object}")
                results.append(False)
                confidence.append(0.0)
                reasoning.append("")
        return {
            'results': results,  # List[bool]: List of evaluation results for each question, True if the predicted answer matches the correct answer, False otherwise
            'confidence': confidence,  # List[float]: List of confidence levels for each evaluation result, between 0 and 1
            'reasoning': reasoning,  # List[str]: List of predicted answers to be evaluated
        }

    def generate_direct_answer(self, question: str, context: str=None, **kwargs):
        """
        Generates a direct answer to a question using the LLM agent.
        
        Args:
            question (str): The question to be answered.
            context (str, optional): Additional context to inform the answer.
        
        Returns:
            dict: A dictionary containing the additional information, reasoning, and direct answer.
        """
        user_message = self.generate_direct_answer_prompt.format(
            examples=self.direct_answer_examples if self.direct_answer_examples else "",
            question=question, 
            context=context if context else "Not provided"
            )
        messages = [{'role': 'user', 'content': user_message}]
        if self.verbose:
            print("*"* 10)
            print(f"Generating direct answer with user message: {user_message}")
        agent_input = {
            'messages': messages,
            'json_schema': generate_direct_answer.DirectAnswerOutput,
            'index': 0
        }
        start_time = time.time()
        responses = self.llm_agent.generate(agent_input, **kwargs)['output']
        if self.verbose:
            print(f"Time taken to generate direct answer: {time.time() - start_time:.2f} seconds")
        answers = []
        detailed_answers = []
        reasoning = []
        additional_information = []
        confidence = []
        for response in responses:
            response_object = response.get('output', None)
            if response_object is not None:
                if isinstance(response_object, generate_direct_answer.DirectAnswerOutput):
                    if self.verbose:
                        print(f"Response object: {response_object}")
                    answers.append(response_object.answer)
                    detailed_answers.append(response_object.detailed_answer)
                    reasoning.append(response_object.reasoning)
                    additional_information.append(response_object.additional_information)
                    answer_confidence = response_object.confidence.lower() if response_object.confidence else None
                    if answer_confidence == 'high':
                        confidence.append(1)
                    elif answer_confidence == 'medium':
                        confidence.append(0.5)
                    else:
                        confidence.append(0)
                else:
                    print(f"Warning: Response object is not of type DirectAnswerOutput: {response_object}")
        if self.verbose:
            print(f"Generated answers: {answers}")
            print(f"Reasoning: {reasoning}")
            print(f"Additional information: {additional_information}")
            print(f"Confidence: {confidence}")
            print(f"Detailed answers: {detailed_answers}")

        assert len(answers) == len(reasoning) == len(confidence), "Answers, reasoning, and confidence lists must be of the same length."
        return {
            'answer': answers, # [str]
            'detailed_answer': detailed_answers,  # [str] Detailed answer with explanations
            'reasoning': reasoning,
            'additional_information': additional_information,
            'confidence': confidence  
        }    
    
    def batch_generate_direct_answer(self, questions: List[str], context: List[str]=None, **kwargs):
        messages = []
        context = context if context is not None else [''] * len(questions)
        assert len(questions) == len(context), "Questions and context must have the same length."
        for q, c in zip(questions, context):
            user_message = self.generate_direct_answer_prompt.format(
                examples=self.direct_answer_examples if self.direct_answer_examples else "",
                question=q, 
                context=c if c else "Not provided"
            )
            messages.append([{'role': 'user', 'content': user_message}])
        start_time = time.time()
        responses = self.llm_agent.batch_generate(batch=messages, response_object=generate_direct_answer.DirectAnswerOutput, n=1, **kwargs)  # For batch processing, do not use multiple sampling
        if self.verbose:
            print(f"Time taken to batch generate direct answers: {time.time() - start_time:.2f} seconds")
        answers = []
        detailed_answers = []
        reasoning = []
        additional_information = []
        confidence = []
        for response in responses:
            response_object = response['output'][0].get('output', None)
            assert response_object is not None, "Response object should not be None"
            if isinstance(response_object, generate_direct_answer.DirectAnswerOutput):
                if self.verbose:
                    print(f"Response object: {response_object}")
                answers.append(response_object.answer)
                detailed_answers.append(response_object.detailed_answer)
                reasoning.append(response_object.reasoning)
                additional_information.append(response_object.additional_information)
                answer_confidence = response_object.confidence.lower() if response_object.confidence else None
                if answer_confidence == 'high':
                    confidence.append(1)
                elif answer_confidence == 'medium':
                    confidence.append(0.5)
                else:
                    confidence.append(0)
            else:
                print(f"Warning: Response object is not of type DirectAnswerOutput: {response_object}")
                answers.append("")
                detailed_answers.append("")
                reasoning.append("")
                additional_information.append("")
                confidence.append(0.0)
        return {
            'answer': answers,  # List[str]: List of direct answers generated for each question
            'detailed_answer': detailed_answers,  # List[str]: List of detailed answers with explanations
            'reasoning': reasoning,  # List[str]: List of reasoning for each answer
            'additional_information': additional_information,  # List[str]: Additional information provided with the answer
            'confidence': confidence  # List[float]: List of confidence levels for each answer, between 0 and 1
        }

    def generate_follow_up_reasoning(self, question: str, context: str=None):
        """
        Generates a single next reasoning step that logically follows from the current reasoning to advance towards a complete answer to the question.
        Args:
            question (str): The question to be answered.
            context (str, optional): Additional context to inform the reasoning.
        Returns:
            dict: A dictionary containing the next reasoning step and its justification.
        """
        user_message = self.generate_one_next_step_prompt.format(
            examples=self.one_next_step_examples if self.one_next_step_examples else "",
            question=question, 
            context=context if context else "Not provided"
            )
        messages = [{'role': 'user', 'content': user_message}]
        if self.verbose:
            print("*"* 10)
            print(f"Generating next step with user message: {user_message}")
        agent_input = {
            'messages': messages,
            'json_schema': generate_one_next_step.OneNextStepOutput,
            'index': 0
        }
        start_time = time.time()
        responses = self.llm_agent.generate(agent_input)['output']
        if self.verbose:
            print(f"Time taken to generate next step: {time.time() - start_time:.2f} seconds")
        next_steps = []
        justifications = []
        answerable_main_question = []
        confidence = []
        should_gather_information = []
        for response in responses:
            response_object = response.get('output', None)
            if response_object is not None:
                if isinstance(response_object, generate_one_next_step.OneNextStepOutput):
                    if self.verbose:
                        print(f"Response object: {response_object}")
                    answerable_main_question.append(response_object.answerable_main_question)
                    next_steps.append(response_object.next_step)
                    justifications.append(response_object.justification)
                    answer_confidence = response_object.confidence.lower() if response_object.confidence else None
                    should_gather_information.append(response_object.should_gather_information)
                    if answer_confidence == 'high':
                        confidence.append(1)
                    elif answer_confidence == 'medium':
                        confidence.append(0.5)
                    else:
                        confidence.append(0)
                else:
                    print(f"Warning: Response object is not of type OneNextStepOutput: {response_object}")
        if self.verbose:
            print(f"Generated next steps: {next_steps}")
            print(f"Justifications: {justifications}")
            print(f"Main question answerable: {answerable_main_question}")
            print(f"Confidence: {confidence}")
            print(f"should_gather_information: {should_gather_information}")
        # Majority voting for the main question answerability
        question_answerable = sum(answerable_main_question) / len(answerable_main_question) >= 0.5 if answerable_main_question else False
        assert len(next_steps) == len(answerable_main_question) == len(justifications), "Next steps, justifications, and answerable main question lists must be of the same length."
        return {
            'next_step': next_steps,  # [str]
            'justification': justifications,
            'main_question_answerable': question_answerable,
            'confidence': confidence,  # [float] Confidence level of the next step, between 0 and 1
            'need_answer': should_gather_information  # [bool] True if the next step requires gathering new information, False otherwise  
        }
    
    def generate_subquestion(self, question: str, context: str=None):
        """
        Generates a subquestion that logically follows from the main question, along with step-by-step reasoning.
        
        Args:
            question (str): The main question to be answered.
            context (str, optional): Additional context to inform the subquestion generation.
        
        Returns:
            dict: A dictionary containing the generated subquestion and reasoning.
        """
        user_message = self.generate_subquestion_prompt.format(
            examples=self.generate_subquestion_examples if self.generate_subquestion_examples else "",
            question=question, 
            context=context if context else "Not provided"
            )
        messages = [{'role': 'user', 'content': user_message}]
        agent_input = {
            'messages': messages,
            'json_schema': generate_subquestion_and_answer.SubquestionOutput,
            'index': 0
        }
        if self.verbose:
            print("*"* 10)
            print(f"Generating subquestion with user message: {user_message}")
        start_time = time.time()
        responses = self.llm_agent.generate(agent_input)['output']
        if self.verbose:
            print(f"Time taken to generate subquestion: {time.time() - start_time:.2f} seconds")
        subquestions = []
        reasoning = []
        main_question_answerable = []
        gap_type = []
        for response in responses:
            response_object = response.get('output', None)
            if response_object is not None:
                if isinstance(response_object, generate_subquestion_and_answer.SubquestionOutput):
                    if self.verbose:
                        print(f"Response object: {response_object}")
                    subquestions.append(response_object.subquestion)
                    main_question_answerable.append(response_object.answerable_main_question)
                    reasoning.append(response_object.reasoning)
                    gap_type.append(response_object.gap_type)
                else:
                    print(f"Warning: Response object is not of type SubquestionOutput: {response_object}")
        # Majority voting for the main question answerability
        if len(main_question_answerable) > 0:
            final_answerable = sum(main_question_answerable) / len(main_question_answerable) >= 0.5
        if self.verbose:
            print(f"Generated subquestions: {subquestions}")
            print(f"Main question answerability: {main_question_answerable}")
            print(f"Main question answerable: {final_answerable}")
            print(f"Reasoning: {reasoning}")
            print(f"Gap types: {gap_type}")
        return {
            'subquestion': subquestions,  # [str]
            'main_question_answerable': final_answerable,  # bool
            'reasoning': reasoning,
            'gap_type': gap_type  # [str] Type of reasoning gap identified, one of 'factual', 'relational', 'causal', 'temporal', 'logical', or 'null'
        }

    def generate_subquestion_and_answer(self, question: str, context: str=None):
        """
        Generates a subquestion and answer that logically follows from the main question, along with step-by-step reasoning.
        
        Args:
            question (str): The main question to be answered.
            context (str, optional): Additional context to inform the subquestion and answer.
        
        Returns:
            dict: A dictionary containing the generated subquestion, direct answer, and reasoning.
        """
        user_message = self.generate_subquestion_and_answer_prompt.format(
            examples=self.subquestion_and_answer_examples if self.subquestion_and_answer_examples else "",
            question=question, 
            context=context if context else "Not provided",
            )
        messages = [{'role': 'user', 'content': user_message}]
        agent_input = {
            'messages': messages,
            'json_schema': generate_subquestion_and_answer.SubquestionAndAnswerOutput,
            'index': 0
        }
        if self.verbose:
            print("*"* 10)
            print(f"Generating subquestion and answer with user message: {user_message}")
        start_time = time.time()
        responses = self.llm_agent.generate(agent_input)['output']
        if self.verbose:
            print(f"Time taken to generate subquestion and answer: {time.time() - start_time:.2f} seconds")
        subquestions = []
        answers = []
        reasoning = []
        confidence = []
        for response in responses:
            response_object = response.get('output', None)
            if response_object is not None:
                if isinstance(response_object, generate_subquestion_and_answer.SubquestionAndAnswerOutput):
                    if self.verbose:
                        print(f"Response object: {response_object}")
                    subquestions.append(response_object.subquestion)
                    answers.append(response_object.answer)
                    reasoning.append(response_object.reasoning)
                    answer_confidence = response_object.confidence.lower() if response_object.confidence else None
                    if answer_confidence == 'high':
                        confidence.append(1)
                    elif answer_confidence == 'medium':
                        confidence.append(0.5)
                    else:
                        confidence.append(0)
                else:
                    print(f"Warning: Response object is not of type SubquestionAndAnswerOutput: {response_object}")
        if self.verbose:
            print(f"Generated subquestions: {subquestions}")
            print(f"Generated answers: {answers}")
            print(f"Reasoning: {reasoning}")
            print(f"Confidence: {confidence}")
        return {
            'subquestion': subquestions,  # [str]
            'answer': answers,  # [str]
            'reasoning': reasoning,
            'confidence': confidence  # [float] Confidence level of the generated answer, between 0 and 1
        }

    def reanswer_subquestion(self, question: str, answer: str, context: str=None):
        """
        Reanswers a subquestion based on the main question and additional context, providing step-by-step reasoning.
        Args:
            question (str): The main question to be answered.
            answer (str): The answer to the subquestion that needs to be reanswered.
            context (str, optional): Additional context to inform the reanswering process.
        Returns:
            dict: A dictionary containing the reanswered subquestion and reasoning.
        """
        user_message = self.reanswer_subquestion_prompt.format(
            examples=self.reanswer_subquestion_examples if self.reanswer_subquestion_examples else "",
            question=question, 
            answer=answer, 
            context=context if context else "Not provided"
            )
        messages = [{'role': 'user', 'content': user_message}]
        if self.verbose:
            print("*"* 10)
            print(f"Reanswering subquestion with user message: {user_message}")
        agent_input = {
            'messages': messages,
            'json_schema': reanswer_subquestion.ReanswerOutput,
            'index': 0
        }
        start_time = time.time()
        responses = self.llm_agent.generate(agent_input)['output']
        if self.verbose:
            print(f"Time taken to reanswer subquestion: {time.time() - start_time:.2f} seconds")
        reanswered_subquestions = []
        reasoning = []
        confidence = []
        for response in responses:
            response_object = response.get('output', None)
            if response_object is not None:
                if isinstance(response_object, reanswer_subquestion.ReanswerOutput):
                    if self.verbose:
                        print(f"Response object: {response_object}")
                    reanswered_subquestions.append(response_object.reanswer)
                    reasoning.append(response_object.reasoning)
                    answer_confidence = response_object.confidence.lower() if response_object.confidence else None
                    if answer_confidence == 'high':
                        confidence.append(1)
                    elif answer_confidence == 'medium':
                        confidence.append(0.5)
                    else:
                        confidence.append(0)
                else:
                    print(f"Warning: Response object is not of type ReanswerOutput: {response_object}")
        if self.verbose:
            print(f"Reanswered subquestions: {reanswered_subquestions}")
            print(f"Reasoning: {reasoning}")
            print(f"Confidence: {confidence}")
        return {
            'reanswered_subquestion': reanswered_subquestions,  # [str]
            'reasoning': reasoning,
            'confidence': confidence  # [float] Confidence level of the reanswered subquestion, between 0 and 1
        }

    def rephase_question(self, question: str):
        """
        Rephrases a question to make it clearer, more specific, or more focused while retaining its original intent.
        
        Args:
            question (str): The question to be rephrased.
        
        Returns:
            dict: A dictionary containing the rephrased question and reasoning.
        """
        user_message = self.rephase_question_prompt.format(
            examples=self.rephase_question_examples if self.rephase_question_examples else "",
            question=question, 
            )
        messages = [{'role': 'user', 'content': user_message}]
        if self.verbose:
            print("*"* 10)
            print(f"Rephrasing question with user message: {user_message}")
        agent_input = {
            'messages': messages,
            'json_schema': rephase_question.RephraseQuestionOutput,
            'index': 0
        }
        start_time = time.time()
        responses = self.llm_agent.generate(agent_input)['output']
        if self.verbose:
            print(f"Time taken to rephrase question: {time.time() - start_time:.2f} seconds")
        rephrased_questions = []
        reasoning = []
        for response in responses:
            response_object = response.get('output', None)
            if response_object is not None:
                if isinstance(response_object, rephase_question.RephraseQuestionOutput):
                    if self.verbose:
                        print(f"Response object: {response_object}")
                    rephrased_questions.append(response_object.rephrased_question)
                    reasoning.append(response_object.reasoning)
                else:
                    print(f"Warning: Response object is not of type RephraseQuestionOutput: {response_object}")
        if self.verbose:
            print(f"Rephrased questions: {rephrased_questions}")
            print(f"Reasoning: {reasoning}")
        return {
            'rephrased_question': rephrased_questions,  # [str]
            'reasoning': reasoning
        }
    
    def generate_queries(self, question: str, context: str=None):
        """
        Generates strategic queries to complement the context for answering the main question.
        
        Args:
            question (str): The main question to be answered.
            context (str, optional): Additional context to inform the query generation.
        
        Returns:
            dict: A dictionary containing the generated queries and reasoning.
        """
        user_message = self.generate_queries_prompt.format(
            examples=self.generate_queries_examples if self.generate_queries_examples else "",
            question=question, 
            context=context if context else "Not provided"
            )
        messages = [{'role': 'user', 'content': user_message}]
        if self.verbose:
            print("*"* 10)
            print(f"Generating queries with user message: {user_message}")
        agent_input = {
            'messages': messages,
            'json_schema': generate_query.QueriesGenerationOutput,
            'index': 0
        }
        start_time = time.time()
        response = self.llm_agent.generate(agent_input, n=1)['output'][0]
        if self.verbose:
            print(f"Time taken to generate queries: {time.time() - start_time:.2f} seconds")
        response_object = response.get('output', None)
        if response_object is not None:
            if isinstance(response_object, generate_query.QueriesGenerationOutput):
                if self.verbose:
                    print(f"Response object: {response_object}")
                queries = response_object.queries
                reasoning = response_object.reasoning
                answerable_main_question = response_object.decision
            else:
                if self.verbose:
                    print(f"Warning: Response object is not of type GenerateQueriesOutput: {response_object}")
                # Extract "answerable_main_question" with regex 
                anserable_main_question_match = re.search(r'"decision":\s*(true|false)', response_object)
                if anserable_main_question_match:
                    answerable_main_question = anserable_main_question_match.group(1)
                    if answerable_main_question == "true":
                        answerable_main_question = True
                    elif answerable_main_question == "false":
                        answerable_main_question = False
                else:
                    answerable_main_question = False
                # Extract the queries
                pattern = r'"queries":\s*\[.*?\],'
                match = re.search(pattern, response_object, re.DOTALL)
                query_str = match.group(0)
                query_str = query_str[len('"queries": '):-2]
                query_str = query_str.strip('[]')
                queries = [query.strip().strip('"') for query in query_str.split(',')]
                reasoning = ""
        return {
            'answerable_main_question': answerable_main_question,  # bool: True if the main question can be answered with the provided context, False otherwise
            'queries': queries,  # [str]
            'reasoning': reasoning
        }

    def batch_generate_queries(self, questions: List[str], context: List[str]=None):
        messages = []
        context = context if context is not None else [''] * len(questions)
        assert len(questions) == len(context), "Questions and context must have the same length."
        for q, c in zip(questions, context):
            user_message = self.generate_queries_prompt.format(
                examples=self.generate_queries_examples if self.generate_queries_examples else "",
                question=q, 
                context=c if c else "Not provided"
            )
            messages.append([{'role': 'user', 'content': user_message}])
        start_time = time.time()
        responses = self.llm_agent.batch_generate(batch=messages, response_object=generate_query.QueriesGenerationOutput, n=1)
        if self.verbose:
            print(f"Time taken to batch generate queries: {time.time() - start_time:.2f} seconds")
        queries = []
        reasoning = []
        answerable_main_question = []
        for response in responses:
            response_object = response['output'][0].get('output', None)
            assert response_object is not None, "Response object should not be None"
            if isinstance(response_object, generate_query.QueriesGenerationOutput):
                if self.verbose:
                    print(f"Response object: {response_object}")
                queries.append(response_object.queries)
                reasoning.append(response_object.reasoning)
                answerable_main_question.append(response_object.decision)
            else:
                print(f"Warning: Response object is not of type GenerateQueriesOutput: {response_object}")
                # Extract "answerable_main_question" with regex
                anserable_main_question_match = re.search(r'"decision":\s*(true|false)', response_object)
                if anserable_main_question_match:
                    answerable_main_question.append(anserable_main_question_match.group(1) == "true")
                else:
                    answerable_main_question.append(False)
                # Extract the queries
                pattern = r'"queries":\s*\[.*?\],'
                match = re.search(pattern, response_object, re.DOTALL)
                query_str = match.group(0)
                query_str = query_str[len('"queries": '):-2]
                query_str = query_str.strip('[]')
                queries.append([query.strip().strip('"') for query in query_str.split(',')])
                reasoning.append("")
        return {
            'answerable_main_question': answerable_main_question,  # [bool] True if the main question can be answered with the provided context, False otherwise
            'queries': queries,  # List[List[str]]: List of queries generated for each question
            'reasoning': reasoning  # List[str]: Reasoning for each query generation
        }
        
    def score_reasoning(self, question: str, reasoning: str, correct_answer: Union[str, List[str]]=None):
        """
        Evaluates the quality of reasoning provided for a question, optionally comparing it to a correct answer.
        
        Args:
            question (str): The question being answered.
            reasoning (str): The reasoning provided for the answer.
            correct_answer (str or List[str], optional): The correct answer to the question for comparison.
        
        Returns:
            dict: A dictionary containing the evaluation result, reasoning, and additional information.
        """
        user_message = self.score_reasoning_prompt.format(
            examples=self.score_reasoning_examples if self.score_reasoning_examples else "",
            original_question=question, 
            reasoning_path=reasoning, 
            correct_answer=correct_answer if correct_answer else ""
            )
        messages = [{'role': 'user', 'content': user_message}]
        if self.verbose:
            print("*"* 10)
            print(f"Scoring reasoning with user message: {user_message}")
        agent_input = {
            'messages': messages,
            'json_schema': evaluate.EvaluateReasoningOutput,
            'index': 0
        }
        start_time = time.time()
        responses = self.llm_agent.generate(agent_input)['output']
        if self.verbose:
            print(f"Time taken to score reasoning: {time.time() - start_time:.2f} seconds")
        step_qualities = []
        overall_qualities = []
        conclusion_qualities = []
        for response in responses:
            response_object = response.get('output', None)
            if response_object is not None:
                if isinstance(response_object, evaluate.EvaluateReasoningOutput):
                    if self.verbose:
                        print(f"Response object: {response_object}")
                    step_quality = response_object.step_quality.lower() if response_object.step_quality else None
                    if step_quality == 'excellent':
                        step_qualities.append(4)
                    elif step_quality == 'good':
                        step_qualities.append(3)
                    elif step_quality == 'fair':
                        step_qualities.append(2)
                    elif step_quality == 'poor':
                        step_qualities.append(1)
                    else:
                        step_qualities.append(0)
                    overall_quality = response_object.overall_quality.lower() if response_object.overall_quality else None
                    if overall_quality == 'excellent':
                        overall_qualities.append(4)
                    elif overall_quality == 'good':
                        overall_qualities.append(3)
                    elif overall_quality == 'fair':
                        overall_qualities.append(2)
                    elif overall_quality == 'poor':
                        overall_qualities.append(1)
                    else:
                        overall_qualities.append(0)
                    conclusion_quality = response_object.conclusion_quality.lower() if response_object.conclusion_quality else None
                    if conclusion_quality == 'excellent':
                        conclusion_qualities.append(4)
                    elif conclusion_quality == 'good':
                        conclusion_qualities.append(3)
                    elif conclusion_quality == 'fair':
                        conclusion_qualities.append(2)
                    elif conclusion_quality == 'poor':
                        conclusion_qualities.append(1)
                    else:
                        conclusion_qualities.append(0)
        step_quality_score = sum(step_qualities) / len(step_qualities) if step_qualities else 0 # Average score for step quality, between 0 and 4
        overall_quality_score = sum(overall_qualities) / len(overall_qualities) if overall_qualities else 0 
        conclusion_quality_score = sum(conclusion_qualities) / len(conclusion_qualities) if conclusion_qualities else 0
        score = (step_quality_score + overall_quality_score + conclusion_quality_score) / 12.0 # Final score between 0 and 1
        if self.verbose:
            print(f"Step quality scores: {step_qualities}")
            print(f"Overall quality scores: {overall_qualities}")
            print(f"Conclusion quality scores: {conclusion_qualities}")
            print(f"Final score: {score:.2f}")
            print(f"Question: {question}")
            print(f"Reasoning: {reasoning}")
            print(f"Correct answer: {correct_answer}")
        return {
            'score': score,  # float: Score of the reasoning, between 0 and 1
            }

    def score_answer(self, question: str, answer: str, context: str=None):
        """
        Evaluates the quality of an answer to a question given a context.
        
        Args:
            question (str): The question being answered.
            answer (str): The answer to be evaluated.
            context (str, optional): Additional context to inform the evaluation.
        
        Returns:
            dict: A dictionary containing the evaluation result, reasoning, and additional information.
        """
        user_message = self.evaluate_answer_given_context_prompt.format(
            examples=self.evaluate_answer_given_context_examples if self.evaluate_answer_given_context_examples else "",
            question=question, 
            answer=answer, 
            context=context if context else "Not provided"
            )
        messages = [{'role': 'user', 'content': user_message}]
        if self.verbose:
            print("*"* 10)
            print(f"Evaluating answer with user message: {user_message}")
        agent_input = {
            'messages': messages,
            'json_schema': evaluate.EvaluateAnswerGivenContextOutput,
            'index': 0
        }
        start_time = time.time()
        responses = self.llm_agent.generate(agent_input)['output']
        if self.verbose:
            print(f"Time taken to evaluate answer given context: {time.time() - start_time:.2f} seconds")
        results = []
        reasoning = []
        for response in responses:
            response_object = response.get('output', None)
            if response_object is not None:
                if isinstance(response_object, evaluate.EvaluateAnswerGivenContextOutput):
                    decision = response_object.decision
                    decision = decision.lower()
                    if decision == 'aligned':
                        results.append(1)
                    elif decision == 'in_conflict':
                        results.append(0)
                    elif decision == 'cannot_be_determined':
                        results.append(0.5)
                    reasoning.append(response_object.reasoning)
                else:
                    print(f"Warning: Response object is not of type EvaluateAnswerGivenContextOutput: {response_object}")
        # Majority voting for the results
        if len(results) > 0:
            score = sum(results) / len(results)  # Calculate the score
            assert 0 <= score <= 1, "Score should be between 0 and 1"
        
        if self.verbose:
            print(f"Final evaluation result: {score}")
            print(f"Question: {question}")
            print(f"Answer: {answer}")
            print(f"Context: {context}")
            print(f"Reasoning: {reasoning}")
        return {
            'score': score,  # float: Score of the answer given the context, between 0 and 1
            'question': question,  # str: The question being answered
            'answer': answer,  # str: The answer to be evaluated
            'reasoning': reasoning  # [str]: Reasoning behind the evaluation
        }

    def extract_information_from_retrieved_docs(self, question: str, document: Union[str,List[str]], current_step_objective: str=""):
        """
        Evaluates the relevance of a retrieved document to a given question and the current step's objective, extracting relevant information if applicable.
        Args:
            question (str): The question being addressed.
            current_step_objective (str): The objective of the current step in the reasoning process.
            document (List[str]): The document to be evaluated, which can be a list of strings or a single string.
        Returns:
            dict: A dictionary containing the evaluation decision, reasoning, and extracted information if the document is relevant.
        """
        if isinstance(document, str):
            document = [document]
        messages = []
        for doc in document:
            user_message = self.evaluated_retrieved_document_prompt.format(
                examples=self.evaluated_retrieved_document_examples if self.evaluated_retrieved_document_examples else "",
                question=question, 
                current_step_objective=current_step_objective,
                document=doc
                )
            messages.append([{'role': 'user', 'content': user_message}])
        if self.verbose:
            print("*"* 10)
            print(f"Evaluating retrieved documents with messages:")
            for i, msg in enumerate(messages):
                print(f"Document {i+1}: {msg[0]['content']}")
        start_time = time.time()
        responses = self.llm_agent.batch_generate(messages, 
                                                  response_object=evaluate.EvaluateRetrievedDocumentOutput,
                                                  n=1) # Don't need to do multiple sampling here
        if self.verbose:
            print(f"Time taken to evaluate retrieved documents: {time.time() - start_time:.2f} seconds")
        results = []
        reasoning = []
        extracted_information = []
        for response in responses: 
            response_object = response['output'][0].get('output', None)
            if response_object is not None:
                if isinstance(response_object, evaluate.EvaluateRetrievedDocumentOutput):
                    if self.verbose:
                        print(f"Response object: {response_object}")
                    decision = response_object.decision.lower()  # Normalize decision to lowercase
                    results.append(decision != 'not_relevant')
                    reasoning.append(response_object.reasoning)
                    extracted_information.append(response_object.extracted_information if decision != 'not_relevant' else "")
                else:
                    print(f"Warning: Response object is not of type EvaluateRetrievedDocumentOutput: {response_object}")
        if self.verbose:
            print(f"Results: {results}")
            print(f"Reasoning: {reasoning}")
            print(f"Extracted Information: {extracted_information}")
        return {
            'decision': results,  # [bool] True if the document is relevant to the question and the current step's objective, False otherwise
            'reasoning': reasoning,
            'extracted_information': extracted_information  # [str] Extracted information from the document that is relevant to the question and the current step's objective or an empty string if the document is not relevant
        }

    def evaluate_same_question(self, question_1: str, question_2: str):
        """
        Evaluates whether two questions are the same or not.
        
        Args:
            question_1 (str): The first question to be compared.
            question_2 (str): The second question to be compared.
        
        Returns:
            dict: A dictionary containing the evaluation decision and reasoning.
        """
        user_message = self.evaluate_same_question_prompt.format(
            examples=self.evaluate_same_question_examples if self.evaluate_same_question_examples else "",
            question_1=question_1, 
            question_2=question_2
            )
        messages = [{'role': 'user', 'content': user_message}]
        if self.verbose:
            print("*"* 10)
            print(f"Evaluating same question with user message: {user_message}")
        agent_input = {
            'messages': messages,
            'json_schema': evaluate.EvaluateSameQuestionOutput,
            'index': 0
        }
        start_time = time.time()
        responses = self.llm_agent.generate(agent_input)['output']
        if self.verbose:
            print(f"Time taken to evaluate same question: {time.time() - start_time:.2f} seconds")
        results = []
        reasoning = []
        for response in responses:
            response_object = response.get('output', None)
            if response_object is not None:
                if isinstance(response_object, evaluate.EvaluateSameQuestionOutput):
                    results.append(response_object.decision)  # True if the two questions are the same, False otherwise
                    reasoning.append(response_object.reasoning)
                else:
                    print(f"Warning: Response object is not of type EvaluateSameQuestionOutput: {response_object}")

        # Majority voting for the results
        if len(results) > 0:
            final_result = sum(results) / len(results) >= 0.5
        else:
            # Print out to help debug the issue
            print(f"Warning: No valid responses received for evaluating same question")
            print(f"user_message: {user_message}")
            print(f"Responses: {responses}")
            final_result = False
        if self.verbose:
            print(f"Final evaluation result: {results}")
            print(f"Reasoning: {reasoning}")
        return {'decision': final_result, 'reasoning': reasoning}

if __name__ == "__main__":
    # Example usage
    online_model_kwargs = {
        'model_name': 'qwen3-8b',
        'url': 'http://n0999.talapas.uoregon.edu:30000/v1',
        'api_key': 'None',
        'concurrency': 64,
    }
    generate_kwargs = {
        'temperature': 0.6,
        'n': 3, # should be odd number s it is used for majority voting
        'top_p': 0.95,
        'max_tokens': 8192,
        'top_k': 20,
        'repetition_penalty': 1.1,
        'logprobs': 1,
        'tensor_parallel_size': 1,
    }

    genrator = Generator(online_model_kwargs=online_model_kwargs, generate_kwargs=generate_kwargs, verbose=True)

    question = "Are director of film Move (1970 Film) and director of film Mditerrane (1963 Film) from the same country?"
    context = "The director of the film Move (1970 Film) is John Smith, who is from the United States. The director of the film Mditerrane (1963 Film) is Jean Dupont, who is from France."
    correct_answer = ["No"]
    predicted_answer = "No, the directors are from different countries."

    batch_questions = [
        "Are director of film Move (1970 Film) and director of film Mditerrane (1963 Film) from the same country?",
        "Who directed the film Move (1970 Film)?",
        "Who directed the film Mditerrane (1963 Film)?"
    ]
    batch_correct_answers = [
        ["No"],
        ["John Smith"],
        ["Jean Dupont"]
    ]
    batch_predicted_answers = [
        "No, the directors are from different countries.",
        "John Smith directed Move (1970 Film).",
        "Jean Dupont directed Mditerrane (1963 Film)."
    ]
    breakpoint()
    queries = genrator.batch_generate_queries(batch_questions)
    breakpoint()
    direct_answers = genrator.batch_generate_direct_answer(batch_questions)
    breakpoint()
    evaluations = genrator.batch_evaluate_answer(batch_questions, batch_correct_answers, batch_predicted_answers)

    # Evaluate the answer
    # evaluation_result = genrator.evaluate_answer(question, correct_answer, predicted_answer)
    # breakpoint()
    # Generate a direct answer
    # direct_answer_result = genrator.generate_direct_answer(question, context)
    # breakpoint()
    # Generate a next reasoning step
    # next_step_result = genrator.generate_follow_up_reasoning(question, context=None)
    # breakpoint()
    # Generate a subquestion
    # subquestion_result = genrator.generate_subquestion(question, context=None)
    # breakpoint()
    # Generate a subquestion and answer
    # subquestion_and_answer_result = genrator.generate_subquestion_and_answer(question, context)
    # breakpoint()
    # Reanswer a subquestion
    # reanswer_result = genrator.reanswer_subquestion(question, predicted_answer, context)
    # breakpoint()
    # Rephrase a question
    # rephrase_result = genrator.rephase_question(question)
    # breakpoint()
    # documents = [
    #     "The director of the film Move (1970 Film) is John Smith, who is from the United States. The director of the film Mditerrane (1963 Film) is Jean Dupont, who is from France",
    #     "John Smith directed Move (1970 Film) and is American. Jean Dupont directed Mditerrane (1963 Film) and is French."
    # ]
    # # Evaluate the retrieved documents
    # retrieved_docs_result = genrator.extract_information_from_retrieved_docs(question, documents, current_step_objective="Determine if the directors are from the same country.")
    # breakpoint()
    # Evaluate if two questions are the same
    # same_question_result = genrator.evaluate_same_question(question, "Are the directors of Move (1970 Film) and Mditerrane (1963 Film) from the same country?")
    # breakpoint()
    # Generate queries to complement the context
    # queries_result = genrator.generate_queries(question, context='')
    # breakpoint()
